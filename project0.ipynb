{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "amateur-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-double",
   "metadata": {},
   "source": [
    "Helper function that displays all the functions that can be invoked on an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "daily-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_object_method(obj):\n",
    "    object_methods = [method_name for method_name in dir(obj)\n",
    "                  if callable(getattr(obj, method_name))]\n",
    "    print(object_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-macintosh",
   "metadata": {},
   "source": [
    "Takes an array of lists and filename as input and creates a json file from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "accessory-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def create_json_file(data, filename):\n",
    "    json_object = dict(data)\n",
    "    json_string = json.dumps(json_object)\n",
    "    open(\"output/%s\" % filename, \"w\").write(json_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-ratio",
   "metadata": {},
   "source": [
    "Computes the top K words from the bag of words for each of the subproject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "peaceful-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_k(words, k=40):\n",
    "    word_frequencies = words.frequencies()\n",
    "    words_with_count_greater_than_two = word_frequencies.filter(lambda x: x[1] > 1 )\n",
    "    top40_words = word_frequencies.topk(k, key=1)\n",
    "    word_counts = top40_words.compute()\n",
    "    print(word_counts)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-suspect",
   "metadata": {},
   "source": [
    "## Subproject 1\n",
    "### Top 40 words across all files\n",
    "- Find top 40 words\n",
    "- No need to filter out stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "living-mandate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "booklist = [\n",
    "    \"handout/data/pg36.txt\",\n",
    "    \"handout/data/pg3207.txt\",\n",
    "    \"handout/data/4300-0.txt\",\n",
    "    \"handout/data/pg19033.txt\",\n",
    "    \"handout/data/pg1497.txt\",\n",
    "    \"handout/data/pg42671.txt\",\n",
    "    \"handout/data/pg514.txt\",\n",
    "    \"handout/data/pg6130.txt\"\n",
    "]\n",
    "word_bag = db.read_text(booklist, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "daily-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowercase_words(word_bag):    \n",
    "    split_words = word_bag.str.split()\n",
    "    stripped_words = split_words.flatten().map(lambda x: x.strip())\n",
    "    word_array = stripped_words.filter(lambda x: x!= \"\")\n",
    "    return word_array.map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "human-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_words = get_lowercase_words(word_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "harmful-defensive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 78844), ('and', 45168), ('of', 44739), ('to', 33436), ('a', 24234), ('in', 22126), ('that', 14818), ('he', 13019), ('is', 12918), ('his', 12270), ('i', 11044), ('with', 10296), ('for', 10036), ('as', 9639), ('be', 8834), ('was', 8787), ('not', 8141), ('it', 8123), ('but', 7856), ('by', 7701), ('or', 7407), ('her', 7403), ('they', 6735), ('which', 6517), ('you', 6354), ('on', 6214), ('from', 5811), ('at', 5695), ('are', 5590), ('she', 5458), ('all', 5437), ('their', 5285), ('have', 5146), ('had', 4647), ('this', 4090), ('my', 3841), ('so', 3710), ('we', 3629), ('no', 3620), ('if', 3571)]\n"
     ]
    }
   ],
   "source": [
    "top_40 = compute_top_k(lowercase_words, k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "designed-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40, \"sp1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-acquisition",
   "metadata": {},
   "source": [
    "## Subproject 2\n",
    "### Top 40 filtered words\n",
    "- Find top 40 words\n",
    "- Filter out the stopwords from the handout file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "amber-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_words(words):\n",
    "    stopwords = open(\"handout/data/stopwords.txt\", \"r\").read().split(\"\\n\")\n",
    "    return words.filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "adapted-reach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 11044), ('not', 8141), ('you', 6354), ('have', 5146), ('no', 3620), ('one', 3498), ('like', 2253), ('more', 2087), ('out', 2021), ('up', 1831), ('man', 1783), ('now', 1579), ('only', 1555), ('must', 1523), ('little', 1485), ('those', 1447), ('good', 1444), ('should', 1417), ('after', 1379), ('great', 1358), ('every', 1356), ('first', 1318), ('own', 1289), ('did', 1271), ('how', 1266), ('see', 1251), ('these', 1244), ('men', 1233), ('over', 1209), ('where', 1205), ('make', 1196), ('upon', 1188), ('nor', 1181), ('never', 1177), ('much', 1167), ('time', 1166), ('said,', 1163), ('two', 1142), ('old', 1140), ('made', 1128)]\n"
     ]
    }
   ],
   "source": [
    "filtered_words = get_filtered_words(lowercase_words)\n",
    "top_40_filtered = compute_top_k(filtered_words, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "governing-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40_filtered, \"sp2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-completion",
   "metadata": {},
   "source": [
    "## Subproject 3\n",
    "### Top 40 words without punctuations\n",
    "- Find top 40 words\n",
    "- Remove leading and trailing punctuation marks\n",
    "- Filter out the stopwords from the handout file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "freelance-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def puntuation_stripper(word):\n",
    "    all_punctuations = \".,:;’!?\"\n",
    "    if word[0] in all_punctuations:\n",
    "        word = word[1:]\n",
    "    if word[-1] in all_punctuations:\n",
    "        word = word[:-1]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "departmental-england",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('not', 8648), ('you', 7230), ('have', 5252), ('one', 3954), ('no', 3871), ('man', 2601), ('more', 2402), ('like', 2374), ('out', 2313), ('up', 2186), ('now', 2104), ('men', 1852), ('good', 1830), ('mr', 1784), ('only', 1703), ('time', 1697), ('god', 1665), ('first', 1645), ('say', 1611), ('must', 1569), ('little', 1551), ('own', 1527), ('those', 1514), ('see', 1494), ('after', 1437), ('great', 1434), ('should', 1429), ('did', 1399), ('us', 1377), ('these', 1366), ('every', 1357), ('before', 1341), ('over', 1340), ('know', 1334), ('how', 1308), ('much', 1307), ('same', 1305), ('where', 1275), ('two', 1263), ('made', 1243)]\n"
     ]
    }
   ],
   "source": [
    "long_words = filtered_words.filter(lambda x: len(x) > 1)\n",
    "words_without_punctuations = long_words.map(puntuation_stripper)\n",
    "\n",
    "filtered_words_without_punctuations = get_filtered_words(words_without_punctuations)\n",
    "top_40_without_punctuations = compute_top_k(filtered_words_without_punctuations, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "blank-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40_without_punctuations, \"sp3.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-highway",
   "metadata": {},
   "source": [
    "## Subproject 4\n",
    "### TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "occupational-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_for_document(document):\n",
    "    word_bag = db.read_text(document)\n",
    "    lowercase_words = get_lowercase_words(word_bag)\n",
    "    long_words = lowercase_words.filter(lambda x: len(x) > 1)\n",
    "    words_without_punctuations = get_words_without_punctuations(long_words)\n",
    "    return get_filtered_words(words_without_punctuations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "employed-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_IDF(words_by_document):\n",
    "    unique_words = []\n",
    "    for words_for_single_document in words_by_document:\n",
    "        unique_words.append(words_for_single_document.distinct())\n",
    "    large_bag = db.concat(unique_words)\n",
    "    frequencies = large_bag.frequencies()\n",
    "    idf = frequencies.map(lambda x: (x[0], math.log(8/x[1])))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stopped-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_TF_IDF(tf, idf):\n",
    "    tf = tf.frequencies()\n",
    "    tf_idf = tf.join(idf, lambda x: x[0], lambda x: x[0]).map(lambda x: (x[0][0], x[0][1] * x[1][1]))\n",
    "    return tf_idf.topk(5, key=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "preliminary-accident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('martians', 330.6312051270939), ('martian', 106.74466580623158), ('woking', 103.97207708399179), ('pit', 77.48551098792638), ('heat-ray', 70.70101241711441), ('soveraign', 1023.0852385064792), ('common-wealth', 867.1271228804915), ('onely', 856.7299151720923), ('civill', 675.8185010459466), ('kingdome', 592.6408393787532), ('stephen', 1000.211381548001), ('bloom', 614.1284019761115), ('j', 365.9817113356511), ('don’t', 361.8228282522914), ('dedalus', 332.71064666877373), ('alice', 230.12486394590184), ('[illustration]', 45.747713916956386), ('rabbit', 29.424877590351784), ('mouse', 23.53990207228143), ('caterpillar', 19.408121055678468), ('republic', 228.73856958478194), ('thrasymachus', 178.8319725844659), ('glaucon', 178.8319725844659), ('plato', 151.81117224637262), ('adeimantus', 149.71979100094816), ('bennet', 576.0053070453145), ('bingley', 494.9070869198009), ('darcy', 474.11267150300256), ('elizabeth', 402.0253647247683), ('jane', 350.7324733633323), ('jo', 1651.0765840937897), ('meg', 1276.7771065914192), ('amy', 1139.53396484055), ('laurie', 1089.627367840234), ('beth', 540.6548008367573), ('hector', 500.4522643642805), ('thy', 437.10337519853414), ('achilles', 391.35087195167876), (\"o'er\", 356.0410188432566), ('jove', 330.53945826495175)]\n"
     ]
    }
   ],
   "source": [
    "book_terms = []\n",
    "for book in booklist:\n",
    "    book_terms.append(get_words_for_document(book))\n",
    "\n",
    "idf = get_IDF(book_terms)\n",
    "tf_idf = []\n",
    "for terms in book_terms:\n",
    "    tf_idf.extend(get_top5_TF_IDF(terms, idf))\n",
    "\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "substantial-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(tf_idf, \"sp4.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
