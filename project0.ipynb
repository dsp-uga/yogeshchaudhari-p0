{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coordinate-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-pledge",
   "metadata": {},
   "source": [
    "Helper function that displays all the functions that can be invoked on an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tight-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_object_method(obj):\n",
    "    object_methods = [method_name for method_name in dir(obj)\n",
    "                  if callable(getattr(obj, method_name))]\n",
    "    print(object_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-imaging",
   "metadata": {},
   "source": [
    "Takes an array of lists and filename as input and creates a json file from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "framed-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def create_json_file(data, filename):\n",
    "    json_object = dict(data)\n",
    "    json_string = json.dumps(json_object)\n",
    "    open(\"output/%s\" % filename, \"w\").write(json_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-province",
   "metadata": {},
   "source": [
    "Computes the top K words from the bag of words for each of the subproject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incoming-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_k(words, k=40):\n",
    "    word_frequencies = words.frequencies()\n",
    "    top40_words = word_frequencies.topk(k, key=1)\n",
    "    word_counts = top40_words.compute()\n",
    "    print(word_counts)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-juvenile",
   "metadata": {},
   "source": [
    "## Subproject 1\n",
    "### Top 40 words across all files\n",
    "- Find top 40 words\n",
    "- No need to filter out stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excited-singapore",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "booklist = [\n",
    "    \"handout/data/pg36.txt\",\n",
    "    \"handout/data/pg3207.txt\",\n",
    "    \"handout/data/4300-0.txt\",\n",
    "    \"handout/data/pg19033.txt\",\n",
    "    \"handout/data/pg1497.txt\",\n",
    "    \"handout/data/pg42671.txt\",\n",
    "    \"handout/data/pg514.txt\",\n",
    "    \"handout/data/pg6130.txt\"\n",
    "]\n",
    "word_bag = db.read_text(booklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "encouraging-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowercase_words(word_bag):    \n",
    "    split_words = word_bag.str.split()\n",
    "    stripped_words = split_words.flatten() #.map(lambda x: x.strip())\n",
    "    word_array = stripped_words.filter(lambda x: x!= \"\")\n",
    "    return word_array.map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "going-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_words = get_lowercase_words(word_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "biblical-bridal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 78837), ('and', 45168), ('of', 44739), ('to', 33436), ('a', 24234), ('in', 22126), ('that', 14818), ('he', 13019), ('is', 12918), ('his', 12270), ('i', 11044), ('with', 10296), ('for', 10036), ('as', 9639), ('be', 8834), ('was', 8787), ('not', 8141), ('it', 8123), ('but', 7856), ('by', 7701), ('or', 7407), ('her', 7403), ('they', 6735), ('which', 6517), ('you', 6354), ('on', 6214), ('from', 5811), ('at', 5695), ('are', 5590), ('she', 5458), ('all', 5437), ('their', 5285), ('have', 5146), ('had', 4647), ('this', 4090), ('my', 3841), ('so', 3710), ('we', 3629), ('no', 3620), ('if', 3571)]\n"
     ]
    }
   ],
   "source": [
    "top_40 = compute_top_k(lowercase_words, k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "secondary-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40, \"sp1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-paris",
   "metadata": {},
   "source": [
    "## Subproject 2\n",
    "### Top 40 filtered words\n",
    "- Find top 40 words\n",
    "- Filter out the stopwords from the handout file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "democratic-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_words(words):\n",
    "    stopwords = open(\"handout/data/stopwords.txt\", \"r\").read().split(\"\\n\")\n",
    "    return words.filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "published-fisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 11044), ('not', 8141), ('you', 6354), ('have', 5146), ('no', 3620), ('one', 3498), ('like', 2253), ('more', 2087), ('out', 2021), ('up', 1831), ('man', 1783), ('now', 1579), ('only', 1555), ('must', 1523), ('little', 1485), ('those', 1447), ('good', 1444), ('should', 1417), ('after', 1379), ('great', 1358), ('every', 1356), ('first', 1318), ('own', 1289), ('did', 1271), ('how', 1266), ('see', 1251), ('these', 1244), ('men', 1233), ('over', 1209), ('where', 1205), ('make', 1196), ('upon', 1188), ('nor', 1181), ('never', 1177), ('much', 1167), ('time', 1166), ('said,', 1163), ('two', 1142), ('old', 1140), ('made', 1128)]\n"
     ]
    }
   ],
   "source": [
    "filtered_words = get_filtered_words(lowercase_words)\n",
    "top_40_filtered = compute_top_k(filtered_words, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "inappropriate-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40_filtered, \"sp2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-burden",
   "metadata": {},
   "source": [
    "## Subproject 3\n",
    "### Top 40 words without punctuations\n",
    "- Find top 40 words\n",
    "- Remove leading and trailing punctuation marks\n",
    "- Filter out the stopwords from the handout file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "emerging-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def puntuation_stripper(word):\n",
    "    all_punctuations = \".,:;'!?\"\n",
    "    word = word[1:] if word[0] in all_punctuations else word\n",
    "    word = word[:-1] if word[-1] in all_punctuations else word\n",
    "    return word\n",
    "\n",
    "def get_words_without_punctuations(words):\n",
    "#     all_punctuations = punctuation + \"â€”\"\n",
    "    all_punctuations = \".,:;'!?\"\n",
    "    return words.map(puntuation_stripper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "closed-private",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('not', 8659), ('you', 7245), ('have', 5256), ('one', 3958), ('no', 3883), ('man', 2602), ('more', 2403), ('like', 2378), ('out', 2313), ('up', 2190), ('now', 2106), ('men', 1855), ('good', 1832), ('mr', 1788), ('only', 1704), ('time', 1697), ('god', 1664), ('first', 1646), ('say', 1611), ('must', 1571), ('little', 1556), ('own', 1528), ('those', 1514), ('see', 1494), ('after', 1437), ('great', 1436), ('should', 1429), ('did', 1400), ('us', 1377), ('these', 1366), ('every', 1361), ('over', 1341), ('before', 1341), ('know', 1334), ('how', 1314), ('much', 1308), ('same', 1305), ('where', 1277), ('two', 1265), ('made', 1244)]\n"
     ]
    }
   ],
   "source": [
    "long_words = filtered_words.filter(lambda x: len(x) > 1)\n",
    "words_without_punctuations = get_words_without_punctuations(long_words)\n",
    "\n",
    "filtered_words = get_filtered_words(words_without_punctuations)\n",
    "top_40_without_punctuations = compute_top_k(filtered_words, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "leading-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40_without_punctuations, \"sp3.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-binary",
   "metadata": {},
   "source": [
    "## Subproject 4\n",
    "### TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "recovered-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_for_document(document):\n",
    "    word_bag = db.read_text(document)\n",
    "    lowercase_words = get_lowercase_words(word_bag)\n",
    "    long_words = lowercase_words.filter(lambda x: len(x) > 1)\n",
    "    words_without_punctuations = get_words_without_punctuations(long_words)\n",
    "    return get_filtered_words(words_without_punctuations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "demonstrated-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_IDF(words_by_document):\n",
    "    unique_words = []\n",
    "    for words_for_single_document in words_by_document:\n",
    "        unique_words.append(words_for_single_document.distinct())\n",
    "    large_bag = db.concat(unique_words)\n",
    "    frequencies = large_bag.frequencies()\n",
    "    idf = frequencies.map(lambda x: (x[0], math.log(8/x[1])))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "silver-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_TF_IDF(tf, idf):\n",
    "    tf = tf.frequencies()\n",
    "    tf_idf = tf.join(idf, lambda x: x[0], lambda x: x[0]).map(lambda x: (x[0][0], x[0][1] * x[1][1]))\n",
    "    return tf_idf.topk(5, key=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_terms = []\n",
    "for book in booklist:\n",
    "    book_terms.append(get_words_for_document(book))\n",
    "\n",
    "idf = get_IDF(book_terms)\n",
    "tf_idf = []\n",
    "for terms in book_terms:\n",
    "    tf_idf.extend(get_top5_TF_IDF(terms, idf))\n",
    "\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(tf_idf, \"sp4.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
