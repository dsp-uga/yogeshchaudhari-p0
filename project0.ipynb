{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "veterinary-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-seller",
   "metadata": {},
   "source": [
    "Helper function that displays all the functions that can be invoked on an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deluxe-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_object_method(obj):\n",
    "    object_methods = [method_name for method_name in dir(obj)\n",
    "                  if callable(getattr(obj, method_name))]\n",
    "    print(object_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-tackle",
   "metadata": {},
   "source": [
    "Takes an array of lists and filename as input and creates a json file from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "catholic-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def create_json_file(data, filename):\n",
    "    json_object = dict(data)\n",
    "    json_string = json.dumps(json_object)\n",
    "    open(\"output/%s\" % filename, \"w\").write(json_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-champion",
   "metadata": {},
   "source": [
    "Computes the top K words from the bag of words for each of the subproject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "north-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_k(words, k=40):\n",
    "    word_frequencies = words.frequencies()\n",
    "    words_with_count_greater_than_two = word_frequencies.filter(lambda x: x[1] > 1 )\n",
    "    top40_words = word_frequencies.topk(k, key=1)\n",
    "    word_counts = top40_words.compute()\n",
    "    print(word_counts)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-union",
   "metadata": {},
   "source": [
    "## Subproject 1\n",
    "### Top 40 words across all files\n",
    "- Find top 40 words\n",
    "- No need to filter out stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atomic-color",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "booklist = [\n",
    "    \"handout/data/pg36.txt\",\n",
    "    \"handout/data/pg3207.txt\",\n",
    "    \"handout/data/4300-0.txt\",\n",
    "    \"handout/data/pg19033.txt\",\n",
    "    \"handout/data/pg1497.txt\",\n",
    "    \"handout/data/pg42671.txt\",\n",
    "    \"handout/data/pg514.txt\",\n",
    "    \"handout/data/pg6130.txt\"\n",
    "]\n",
    "word_bag = db.read_text(booklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interim-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowercase_words(word_bag):    \n",
    "    split_words = word_bag.str.split()\n",
    "    stripped_words = split_words.flatten().map(lambda x: x.strip())\n",
    "    word_array = stripped_words.filter(lambda x: x!= \"\")\n",
    "    return word_array.map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "threaded-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_words = get_lowercase_words(word_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "welcome-white",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 78837), ('and', 45168), ('of', 44739), ('to', 33436), ('a', 24234), ('in', 22126), ('that', 14818), ('he', 13019), ('is', 12918), ('his', 12270), ('i', 11044), ('with', 10296), ('for', 10036), ('as', 9639), ('be', 8834), ('was', 8787), ('not', 8141), ('it', 8123), ('but', 7856), ('by', 7701), ('or', 7407), ('her', 7403), ('they', 6735), ('which', 6517), ('you', 6354), ('on', 6214), ('from', 5811), ('at', 5695), ('are', 5590), ('she', 5458), ('all', 5437), ('their', 5285), ('have', 5146), ('had', 4647), ('this', 4090), ('my', 3841), ('so', 3710), ('we', 3629), ('no', 3620), ('if', 3571)]\n"
     ]
    }
   ],
   "source": [
    "top_40 = compute_top_k(lowercase_words, k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intimate-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40, \"sp1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-bathroom",
   "metadata": {},
   "source": [
    "## Subproject 2\n",
    "### Top 40 filtered words\n",
    "- Find top 40 words\n",
    "- Filter out the stopwords from the handout file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "leading-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_words(words):\n",
    "    stopwords = open(\"handout/data/stopwords.txt\", \"r\").read().split(\"\\n\")\n",
    "    return words.filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secret-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 11044), ('not', 8141), ('you', 6354), ('have', 5146), ('no', 3620), ('one', 3498), ('like', 2253), ('more', 2087), ('out', 2021), ('up', 1831), ('man', 1783), ('now', 1579), ('only', 1555), ('must', 1523), ('little', 1485), ('those', 1447), ('good', 1444), ('should', 1417), ('after', 1379), ('great', 1358), ('every', 1356), ('first', 1318), ('own', 1289), ('did', 1271), ('how', 1266), ('see', 1251), ('these', 1244), ('men', 1233), ('over', 1209), ('where', 1205), ('make', 1196), ('upon', 1188), ('nor', 1181), ('never', 1177), ('much', 1167), ('time', 1166), ('said,', 1163), ('two', 1142), ('old', 1140), ('made', 1128)]\n"
     ]
    }
   ],
   "source": [
    "filtered_words = get_filtered_words(lowercase_words)\n",
    "top_40_filtered = compute_top_k(filtered_words, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dated-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40_filtered, \"sp2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-discrimination",
   "metadata": {},
   "source": [
    "## Subproject 3\n",
    "### Top 40 words without punctuations\n",
    "- Find top 40 words\n",
    "- Remove leading and trailing punctuation marks\n",
    "- Filter out the stopwords from the handout file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-trainer",
   "metadata": {},
   "source": [
    "For the `all_punctuations`, the symbol `’` if used as a single quote gets better score on the autolab auto grader. \n",
    "\n",
    "The symbol `'` yeilds poorer result, but the values seems to be closer to the actual expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ahead-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def puntuation_stripper(word):\n",
    "#     all_punctuations = \".,:;’!?\"\n",
    "    all_punctuations = \".,:;'!?\"\n",
    "    if word[0] in all_punctuations:\n",
    "        word = word[1:]\n",
    "    if word[-1] in all_punctuations:\n",
    "        word = word[:-1]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "worst-doctor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('not', 8659), ('you', 7245), ('have', 5256), ('one', 3958), ('no', 3883), ('man', 2602), ('more', 2403), ('like', 2378), ('out', 2313), ('up', 2190), ('now', 2106), ('men', 1855), ('good', 1832), ('mr', 1788), ('only', 1704), ('time', 1697), ('god', 1664), ('first', 1646), ('say', 1611), ('must', 1571), ('little', 1556), ('own', 1528), ('those', 1514), ('see', 1494), ('after', 1437), ('great', 1436), ('should', 1429), ('did', 1400), ('us', 1377), ('these', 1366), ('every', 1361), ('over', 1341), ('before', 1341), ('know', 1334), ('how', 1314), ('much', 1308), ('same', 1305), ('where', 1277), ('two', 1265), ('made', 1244)]\n"
     ]
    }
   ],
   "source": [
    "long_words = filtered_words.filter(lambda x: len(x) > 1)\n",
    "words_without_punctuations = long_words.map(puntuation_stripper)\n",
    "\n",
    "filtered_words_without_punctuations = get_filtered_words(words_without_punctuations)\n",
    "top_40_without_punctuations = compute_top_k(filtered_words_without_punctuations, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "polyphonic-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(top_40_without_punctuations, \"sp3.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-fitness",
   "metadata": {},
   "source": [
    "## Subproject 4\n",
    "### TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "advanced-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_for_document(document):\n",
    "    word_bag = db.read_text(document)\n",
    "    lowercase_words = get_lowercase_words(word_bag)\n",
    "    long_words = lowercase_words.filter(lambda x: len(x) > 1)\n",
    "    words_without_punctuations = long_words.map(puntuation_stripper)\n",
    "    return get_filtered_words(words_without_punctuations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "improved-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_IDF(words_by_document):\n",
    "    unique_words = []\n",
    "    for words_for_single_document in words_by_document:\n",
    "        unique_words.append(words_for_single_document.distinct())\n",
    "    large_bag = db.concat(unique_words)\n",
    "    frequencies = large_bag.frequencies()\n",
    "    idf = frequencies.map(lambda x: (x[0], math.log(8/x[1])))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "played-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_TF_IDF(tf, idf):\n",
    "    tf = tf.frequencies()\n",
    "    tf_idf = tf.join(idf, lambda x: x[0], lambda x: x[0]).map(lambda x: (x[0][0], x[0][1] * x[1][1]))\n",
    "    return tf_idf.topk(5, key=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reverse-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('martians', 330.6312051270939), ('martian', 106.74466580623158), ('woking', 103.97207708399179), ('pit', 77.48551098792638), ('heat-ray', 70.70101241711441), ('soveraign', 1023.0852385064792), ('common-wealth', 867.1271228804915), ('onely', 856.7299151720923), ('civill', 675.8185010459466), ('kingdome', 592.6408393787532), ('stephen', 1000.211381548001), ('bloom', 614.1284019761115), ('j', 365.9817113356511), ('don’t', 361.8228282522914), ('dedalus', 332.71064666877373), ('alice', 230.12486394590184), ('[illustration]', 45.747713916956386), ('rabbit', 29.424877590351784), ('mouse', 23.53990207228143), ('caterpillar', 19.408121055678468), ('republic', 228.73856958478194), ('thrasymachus', 178.8319725844659), ('glaucon', 178.8319725844659), ('plato', 151.81117224637262), ('adeimantus', 149.71979100094816), ('bennet', 576.0053070453145), ('bingley', 494.9070869198009), ('darcy', 474.11267150300256), ('elizabeth', 402.0253647247683), ('jane', 350.7324733633323), ('jo', 1651.0765840937897), ('meg', 1276.7771065914192), ('amy', 1139.53396484055), ('laurie', 1089.627367840234), ('beth', 540.6548008367573), ('hector', 500.4522643642805), ('thy', 437.10337519853414), ('achilles', 391.35087195167876), (\"o'er\", 356.0410188432566), ('jove', 330.53945826495175)]\n"
     ]
    }
   ],
   "source": [
    "book_terms = []\n",
    "for book in booklist:\n",
    "    book_terms.append(get_words_for_document(book))\n",
    "\n",
    "idf = get_IDF(book_terms)\n",
    "tf_idf = []\n",
    "for terms in book_terms:\n",
    "    tf_idf.extend(get_top5_TF_IDF(terms, idf))\n",
    "\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "transparent-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_json_file(tf_idf, \"sp4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
